# ä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•çº é”™

[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/TW-NLP/ChineseErrorCorrector/blob/main/README.md)   [**English**](https://github.com/TW-NLP/ChineseErrorCorrector/blob/main/README_EN.md)

<div align="center">
  <a href="https://github.com/TW-NLP/ChineseErrorCorrector">
    <img src="images/image_fx_.jpg" alt="Logo" height="156">
  </a>
</div>



-----------------

## ä»‹ç»

æ”¯æŒä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•é”™è¯¯çº æ­£ï¼Œå¹¶å¼€æº[æ‹¼å†™å’Œè¯­æ³•é”™è¯¯çš„å¢å¼ºå·¥å…·](https://github.com/TW-NLP/ChineseErrorCorrector/tree/0.1.0)ã€å¤§æ¨¡å‹è®­ç»ƒä»£ç ã€[æ–‡æœ¬çº é”™ç›¸å…³è®ºæ–‡](https://github.com/TW-NLP/ChineseErrorCorrector/blob/main/README_paper.md)å’Œ[é€šç”¨çš„æ–‡æœ¬çº é”™è¯„æµ‹å·¥å…·](https://github.com/TW-NLP/ChineseErrorCorrector/blob/main/ChineseErrorCorrector/scores/README.md)ã€‚è£è·2024CCL å† å†›
ğŸ†ï¼Œ[æŸ¥çœ‹è®ºæ–‡](https://aclanthology.org/2024.ccl-3.31/) ï¼Œ[2023 NLPCC-NaCGECçº é”™å† å†›ğŸ†](https://github.com/TW-NLP/ChineseErrorCorrector?tab=readme-ov-file#nacgec-%E6%95%B0%E6%8D%AE%E9%9B%86)ï¼Œ [2022 FCGEC çº é”™å† å†›ğŸ†](https://github.com/TW-NLP/ChineseErrorCorrector?tab=readme-ov-file#fcgec-%E6%95%B0%E6%8D%AE%E9%9B%86)
ï¼Œå¦‚æœ‰å¸®åŠ©ï¼Œæ„Ÿè°¢starâœ¨ã€‚

## ğŸ”¥ğŸ”¥ğŸ”¥ æ–°é—»
[2025/08/08] å‘å¸ƒ[é€šç”¨çš„æ–‡æœ¬è¯„æµ‹å·¥å…·-Common Errantï¼ˆæ”¯æŒ80ç§è¯­è¨€ï¼‰](https://github.com/TW-NLP/ChineseErrorCorrector/blob/main/ChineseErrorCorrector/scores/README.md) ğŸ‰ï¼Œå¯ä»¥åœ¨é«˜ï¼ˆä¸­æ–‡ã€è‹±æ–‡ï¼‰ã€ä½èµ„æºï¼ˆå°åœ°è¯­ã€å­ŸåŠ æ‹‰è¯­ç­‰ï¼‰ä¸Šè¿›è¡Œæ–‡æœ¬çº é”™çš„è¯„æµ‹ã€‚

[2025/08/06] å‘å¸ƒ[æ–‡æœ¬çº é”™ç›¸å…³è®ºæ–‡ï¼ˆæŒç»­æ›´æ–°ç‰ˆï¼‰](https://github.com/TW-NLP/ChineseErrorCorrector/blob/main/README_paper.md) ğŸ¥³ï¼Œæ–¹ä¾¿å¤§å®¶è¿›è¡Œç ”å­¦ã€‚

[2025/08/01] å‘å¸ƒ[twnlp/ChineseErrorCorrector3-4B](https://huggingface.co/twnlp/ChineseErrorCorrector3-4B) ğŸ‰ğŸ‰ğŸ‰ï¼Œæ³›åŒ–æ€§å…¨é¢æå‡ï¼Œåœ¨å¼€æºçš„æ‰€æœ‰æ¨¡å‹ä¸­ï¼Œä½åˆ—ç¬¬ä¸€ï¼Œ[æ¦œå•è¯¦æƒ…](https://github.com/TW-NLP/ChineseErrorCorrector?tab=readme-ov-file#evaluation%E6%B3%9B%E5%8C%96%E6%80%A7%E8%AF%84%E6%B5%8B)ã€‚

[2025/05/01] æ ¹æ®[å»ºè®®](https://github.com/TW-NLP/ChineseErrorCorrector/issues/17)
ï¼Œæˆ‘ä»¬é‡æ–°è®­ç»ƒçº é”™æ¨¡å‹(ChineseErrorCorrector2-7B)ï¼Œå¹¶å®Œå…¨å¼€æºè®­ç»ƒæ­¥éª¤ï¼Œæ”¯æŒç»“æœå¤ç°ï¼Œ[å¤ç°æ•™ç¨‹](https://github.com/TW-NLP/ChineseErrorCorrector/tree/v0.4.0?tab=readme-ov-file#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E5%A4%8D%E7%8E%B0)

[2025/03/17]
æ›´æ–°æ‰¹é‡é”™è¯¯æ–‡æœ¬çš„è§£æï¼Œ[transformersæ‰¹é‡è§£æ](https://github.com/TW-NLP/ChineseErrorCorrector?tab=readme-ov-file#transformers-%E6%89%B9%E9%87%8F%E6%8E%A8%E7%90%86) ;[VLLMæ‰¹é‡è§£æ](https://github.com/TW-NLP/ChineseErrorCorrector?tab=readme-ov-file#vllm-%E5%BC%82%E6%AD%A5%E6%89%B9%E9%87%8F%E6%8E%A8%E7%90%86)

[2025/03/10] æ¨¡å‹æ”¯æŒå¤šç§æ¨ç†æ–¹å¼ï¼ŒåŒ…æ‹¬ transformersã€VLLMã€modelscopeã€‚

[2025/02/25]
ğŸ‰ğŸ‰ğŸ‰ä½¿ç”¨200ä¸‡çº é”™æ•°æ®è¿›è¡Œå¤šè½®è¿­ä»£è®­ç»ƒï¼Œå‘å¸ƒäº†[twnlp/ChineseErrorCorrector2-7B](https://huggingface.co/twnlp/ChineseErrorCorrector2-7B)
ï¼Œåœ¨ [NaCGEC-2023NLPCCå®˜æ–¹è¯„æµ‹æ•°æ®é›†](https://github.com/masr2000/NaCGEC)
ä¸Šï¼Œè¶…è¶Šç¬¬ä¸€ååä¸º10ä¸ªç‚¹ï¼Œé¥é¥é¢†å…ˆï¼Œ [æŠ€æœ¯è¯¦æƒ…](https://blog.csdn.net/qq_43765734/article/details/145858955)

[2025/02]
ä¸ºæ–¹ä¾¿éƒ¨ç½²ï¼Œä½¿ç”¨38ä¸‡å¼€æºæ‹¼å†™æ•°æ®ï¼Œå‘å¸ƒäº†[twnlp/ChineseErrorCorrector-1.5B](https://huggingface.co/twnlp/ChineseErrorCorrector-1.5B)

[2025/01]
ä½¿ç”¨38ä¸‡å¼€æºæ‹¼å†™æ•°æ®ï¼ŒåŸºäºQwen2.5è®­ç»ƒä¸­æ–‡æ‹¼å†™çº é”™æ¨¡å‹ï¼Œæ”¯æŒè¯­ä¼¼ã€å½¢ä¼¼ç­‰é”™è¯¯çº æ­£ï¼Œå‘å¸ƒäº†[twnlp/ChineseErrorCorrector-7B](https://huggingface.co/twnlp/ChineseErrorCorrector-7B)ï¼Œ[twnlp/ChineseErrorCorrector-32B-LORA](https://huggingface.co/twnlp/ChineseErrorCorrector-32B-LORA/tree/main)

[2024/06]
v0.1.0ç‰ˆæœ¬ï¼šğŸ‰ğŸ‰ğŸ‰å¼€æºä¸€é”®è¯­æ³•é”™è¯¯å¢å¼ºå·¥å…·ï¼Œè¯¥å·¥å…·å¯ä»¥è¿›è¡Œ14ç§è¯­æ³•é”™è¯¯çš„å¢å¼ºï¼Œä¸åŒè¡Œä¸šå¯ä»¥æ ¹æ®è‡ªå·±çš„æ•°æ®è¿›è¡Œé”™è¯¯æ›¿æ¢ï¼Œæ¥è®­ç»ƒè‡ªå·±çš„è¯­æ³•å’Œæ‹¼å†™æ¨¡å‹ã€‚è¯¦è§[Tag-v0.1.0](https://github.com/TW-NLP/ChineseErrorCorrector/tree/0.1.0)

## ğŸ¯æ¨¡å‹åˆ—è¡¨

| æ¨¡å‹åç§°                                                                                        | çº é”™ç±»å‹  | æè¿°                                         |
|:--------------------------------------------------------------------------------------------|:------|:-------------------------------------------|
| [twnlp/ChineseErrorCorrector3-4B](https://huggingface.co/twnlp/ChineseErrorCorrector3-4B)   | è¯­æ³•+æ‹¼å†™ | ä½¿ç”¨200ä¸‡çº é”™æ•°æ®è¿›è¡Œå…¨é‡è®­ç»ƒï¼Œé€‚ç”¨äºè¯­æ³•çº é”™å’Œæ‹¼å†™çº é”™ï¼Œæ•ˆæœæœ€å¥½ï¼Œæ¨èä½¿ç”¨ã€‚   |
| [twnlp/ChineseErrorCorrector2-7B](https://huggingface.co/twnlp/ChineseErrorCorrector2-7B)   | è¯­æ³•+æ‹¼å†™ | ä½¿ç”¨200ä¸‡çº é”™æ•°æ®è¿›è¡Œå¤šè½®è¿­ä»£è®­ç»ƒï¼Œé€‚ç”¨äºè¯­æ³•çº é”™å’Œæ‹¼å†™çº é”™ï¼Œæ•ˆæœè¾ƒå¥½ã€‚ |
| [twnlp/ChineseErrorCorrector-7B](https://huggingface.co/twnlp/ChineseErrorCorrector-7B)     | æ‹¼å†™    | ä½¿ç”¨38ä¸‡å¼€æºæ‹¼å†™æ•°æ®ï¼Œæ”¯æŒè¯­ä¼¼ã€å½¢ä¼¼ç­‰æ‹¼å†™é”™è¯¯çº æ­£ï¼Œæ‹¼å†™çº é”™æ•ˆæœå¥½ã€‚        |
| [twnlp/ChineseErrorCorrector-1.5B](https://huggingface.co/twnlp/ChineseErrorCorrector-1.5B) | æ‹¼å†™    | ä½¿ç”¨38ä¸‡å¼€æºæ‹¼å†™æ•°æ®ï¼Œæ”¯æŒè¯­ä¼¼ã€å½¢ä¼¼ç­‰æ‹¼å†™é”™è¯¯çº æ­£ï¼Œæ‹¼å†™çº é”™æ•ˆæœä¸€èˆ¬ã€‚       |

## ğŸ“Šæ•°æ®é›†

| æ•°æ®é›†åç§°                        | æ•°æ®é“¾æ¥                                                                                             | æ•°æ®é‡å’Œç±»åˆ«è¯´æ˜                                                                 | æè¿°                              |
|:-----------------------------|:-------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:--------------------------------|
| ChinseseErrorCorrectData     | [twnlp/ChinseseErrorCorrectData](https://huggingface.co/datasets/twnlp/ChinseseErrorCorrectData) | 200ä¸‡                                                                     | ChineseErrorCorrector è®­ç»ƒæ•°æ®é›† |
| CSCï¼ˆæ‹¼å†™çº é”™æ•°æ®é›†ï¼‰                 | [twnlp/csc_data](https://huggingface.co/datasets/twnlp/csc_data)                                 | W271K(279,816) Medical(39,303) Lemon(22,259) ECSpell(6,688) CSCD(35,001) | ä¸­æ–‡æ‹¼å†™çº é”™çš„æ•°æ®é›†                      |
| CGCï¼ˆè¯­æ³•çº é”™æ•°æ®é›†ï¼‰                 | [twnlp/cgc_data](https://huggingface.co/datasets/twnlp/cgc_data)                                 | CGED(20,449) FCGEC(37,354) MuCGEC(2,467) NaCGEC(7,568)                   | ä¸­æ–‡è¯­æ³•çº é”™çš„æ•°æ®é›†                      |
| Lang8+HSKï¼ˆç™¾ä¸‡è¯­æ–™-æ‹¼å†™å’Œè¯­æ³•é”™è¯¯æ··åˆæ•°æ®é›†ï¼‰ | [twnlp/lang8_hsk](https://huggingface.co/datasets/twnlp/lang8_hsk)                               | 1,568,885                                                                | ä¸­æ–‡æ‹¼å†™å’Œè¯­æ³•æ•°æ®é›†                      |


##  ğŸ—ï¸ Evaluation


### æ³›åŒ–æ€§è¯„æµ‹
- è¯„ä¼°æŒ‡æ ‡ï¼šF1
- CSC(Chinese Spelling Correction): æ‹¼å†™çº é”™æ¨¡å‹ï¼Œè¡¨ç¤ºæ¨¡å‹å¯ä»¥å¤„ç†éŸ³ä¼¼ã€å½¢ä¼¼ã€è¯­æ³•ç­‰é•¿åº¦å¯¹é½çš„é”™è¯¯çº æ­£
- CTC(CHinese Text Correction): æ–‡æœ¬çº é”™æ¨¡å‹ï¼Œè¡¨ç¤ºæ¨¡å‹æ”¯æŒæ‹¼å†™ã€è¯­æ³•ç­‰é•¿åº¦å¯¹é½çš„é”™è¯¯çº æ­£ï¼Œè¿˜å¯ä»¥å¤„ç†å¤šå­—ã€å°‘å­—ç­‰é•¿åº¦ä¸å¯¹é½çš„é”™è¯¯çº æ­£
- GPUï¼šTesla V100ï¼Œæ˜¾å­˜ 32 GB

| Model Name        | Model Link                                                                                                              | Base Model                     | Avg        | SIGHAN-2015 | EC-LAW | MCSC   | GPU | QPS     |
|:------------------|:------------------------------------------------------------------------------------------------------------------------|:-------------------------------|:-----------|:------------|:-------|:-------|:--------|:--------|
| Kenlm-CSC         | [shibing624/chinese-kenlm-klm](https://huggingface.co/shibing624/chinese-kenlm-klm)                                     | kenlm                          | 0.3409     | 0.3147      | 0.3763 | 0.3317 | CPU     | 9       |
| Mengzi-T5-CSC     | [shibing624/mengzi-t5-base-chinese-correction](https://huggingface.co/shibing624/mengzi-t5-base-chinese-correction)     | mengzi-t5-base                 | 0.3984     | 0.7758      | 0.3156 | 0.1039 | GPU     | 214     |
| ERNIE-CSC         | [PaddleNLP/ernie-csc](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/legacy/examples/text_correction/ernie-csc) | PaddlePaddle/ernie-1.0-base-zh | 0.4353     | 0.8383      | 0.3357 | 0.1318 | GPU     | 114     |
| MacBERT-CSC       | [shibing624/macbert4csc-base-chinese](https://huggingface.co/shibing624/macbert4csc-base-chinese)                       | hfl/chinese-macbert-base       | 0.3993     | 0.8314      | 0.1610 | 0.2055 | GPU     | **224** |
| ChatGLM3-6B-CSC   | [shibing624/chatglm3-6b-csc-chinese-lora](https://huggingface.co/shibing624/chatglm3-6b-csc-chinese-lora)               | THUDM/chatglm3-6b              | 0.4538     | 0.6572      | 0.4369 | 0.2672 | GPU     | 3       |
| Qwen2.5-1.5B-CTC  | [shibing624/chinese-text-correction-1.5b](https://huggingface.co/shibing624/chinese-text-correction-1.5b)               | Qwen/Qwen2.5-1.5B-Instruct     | 0.6802     | 0.3032      | 0.7846 | 0.9529 | GPU     | 6       |
| Qwen2.5-7B-CTC    | [shibing624/chinese-text-correction-7b](https://huggingface.co/shibing624/chinese-text-correction-7b)                   | Qwen/Qwen2.5-7B-Instruct       | 0.8225     | 0.4917      | 0.9798 | 0.9959 | GPU     | 3       |
| **Qwen3-4B-CTC(Our)** | [twnlp/ChineseErrorCorrector3-4B](https://huggingface.co/twnlp/ChineseErrorCorrector3-4B)                   | Qwen/Qwen3-4B                  | **0.8521** | 0.6340      | 0.9360 | 0.9864 | GPU     | 5       |

### è¯­æ³•çº é”™(åŒå† å†› ğŸ†)

#### NaCGEC æ•°æ®é›† ğŸ†

- è¯„ä¼°å·¥å…·ï¼šChERRANT  [è¯„æµ‹å·¥å…·](https://github.com/HillZhang1999/MuCGEC)
- è¯„ä¼°æ•°æ®ï¼š[NaCGEC](https://github.com/masr2000/NaCGEC)
- è¯„ä¼°æŒ‡æ ‡ï¼šF1-0.5

| Model Name | Model Link | Prec | Rec | F0.5 |
|:-----------------|:---------------------------------------------------------------|:-----------|:------------|:-------|
| twnlp/ChineseErrorCorrector3-4B | [huggingface](https://huggingface.co/twnlp/ChineseErrorCorrector3-4B) ï¼› [modelspose(å›½å†…ä¸‹è½½)](https://www.modelscope.cn/models/tiannlp/ChineseErrorCorrector3-4B) | 0.542 | 0.3475 | 0.4874 |
| HW_TSC_nlpcc2023_cgec(åä¸º) | æœªå¼€æº | 0.5095 | 0.3129 | 0.4526 |
| é±¼é¥¼å•¾å•¾Plus(åŒ—äº¬å¤§å­¦) | æœªå¼€æº | 0.5708 | 0.1294 | 0.3394 |
| CUHK_SU(é¦™æ¸¯ä¸­æ–‡å¤§å­¦) | æœªå¼€æº | 0.3882 | 0.1558 | 0.2990 |

####  FCGEC æ•°æ®é›† ğŸ†

- è¯„ä¼°æŒ‡æ ‡ï¼šbinary_f1

- [è¯„æµ‹ğŸ”—](https://codalab.lisn.upsaclay.fr/competitions/8020#results)

## ğŸš€ å¿«é€Ÿå¼€å§‹
<div align="center">
  <img src="https://user-images.githubusercontent.com/74038190/212284158-e840e285-664b-44d7-b79b-e264b5e54825.gif" width="400">
</div>

### ğŸ¤— transformers

```shell
pip install transformers
```

```shell
from transformers import AutoModelForCausalLM, AutoTokenizer,set_seed
set_seed(42)

model_name = "twnlp/ChineseErrorCorrector3-4B"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬çº é”™ä¸“å®¶ï¼Œçº æ­£è¾“å…¥å¥å­ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶è¾“å‡ºæ­£ç¡®çš„å¥å­ï¼Œè¾“å…¥å¥å­ä¸ºï¼š"
text_input = "å¯¹å¾…æ¯ä¸€é¡¹å·¥ä½œéƒ½è¦ä¸€ä¸ä¸å¤Ÿã€‚"
messages = [
    {"role": "user", "content": prompt + text_input}
]
text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.
    )
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)

```

### VLLM

```shell
pip install transformers
pip install vllm==0.8.5
```

```shell
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained("twnlp/ChineseErrorCorrector3-4B")

# Pass the default decoding hyperparameters of twnlp/ChineseErrorCorrector3-4B
# max_tokens is for the maximum length for generation.
sampling_params = SamplingParams(seed=42,max_tokens=512)

# Input the model name or path. Can be GPTQ or AWQ models.
llm = LLM(model="twnlp/ChineseErrorCorrector3-4B")

# Prepare your prompts
text_input = "å¯¹å¾…æ¯ä¸€é¡¹å·¥ä½œéƒ½è¦ä¸€ä¸ä¸å¤Ÿã€‚"
messages = [
    {"role": "user", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬çº é”™ä¸“å®¶ï¼Œçº æ­£è¾“å…¥å¥å­ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶è¾“å‡ºæ­£ç¡®çš„å¥å­ï¼Œè¾“å…¥å¥å­ä¸ºï¼š"+text_input}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=False
)

# generate outputs
outputs = llm.generate([text], sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}") 
```

### ğŸ‘ VLLM å¼‚æ­¥æ‰¹é‡æ¨ç†(å·¥ç¨‹æ¨è)

- Clone the repo

``` sh
git clone https://github.com/TW-NLP/ChineseErrorCorrector
cd ChineseErrorCorrector
```

- Install Conda: please see https://docs.conda.io/en/latest/miniconda.html
- Create Conda env:

``` sh
conda create -n zh_correct -y python=3.10
conda activate zh_correct
pip install -r requirements.txt
# If you are in mainland China, you can set the mirror as follows:
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
```
æ–¹æ³•ä¸€ï¼šç›´æ¥å¯åŠ¨VLLMæœåŠ¡ï¼Œä½¿ç”¨openaiæ¥å£è¿›è¡Œè°ƒç”¨ï¼Œåªè¾“å‡ºæ­£ç¡®è¯­å¥ï¼Œéƒ¨ç½²è„šæœ¬ï¼š
``` sh
CUDA_VISIBLE_DEVICES=0 nohup vllm serve twnlp/ChineseErrorCorrector3-4B \
    --port 8000 \
    --max-model-len 1024 \
    --gpu-memory-utilization 0.9 \
    --seed 42 \
    >chinese_corrector.log 2>&1 &
```
cURL è°ƒç”¨ï¼ˆopenaiæ ¼å¼ï¼‰
``` sh
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "twnlp/ChineseErrorCorrector3-4B",
    "messages": [
      {
        "role": "user",
        "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬çº é”™ä¸“å®¶ï¼Œçº æ­£è¾“å…¥å¥å­ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶è¾“å‡ºæ­£ç¡®çš„å¥å­ï¼Œè¾“å…¥å¥å­ä¸ºï¼šå¯¹å¾…æ¯ä¸€é¡¹å·¥ä½œéƒ½è¦ä¸€ä¸ä¸å¤Ÿã€‚"
      }
    ],
    "max_tokens": 1024,
    "temperature": 0,
    "seed": 42
  }'
```

æ–¹æ³•äºŒï¼šä½¿ç”¨ChineseErrorCorrectoræ¨ç†ï¼Œæœ‰å¯¹åº”çš„åå¤„ç†æ“ä½œã€‚
```sh
# ä¿®æ”¹config.py
#ï¼ˆ1ï¼‰æ ¹æ®ä¸åŒçš„æ¨¡å‹ï¼Œä¿®æ”¹çš„DEFAULT_CKPT_PATHï¼Œé»˜è®¤ä¸ºtwnlp/ChineseErrorCorrector3-4B(å°†æ¨¡å‹ä¸‹è½½ï¼Œæ”¾åœ¨ChineseErrorCorrector/pre_model/twnlp/ChineseErrorCorrector3-4B)
#ï¼ˆ2ï¼‰å°†TextCorrectConfigçš„USE_VLLM = True

#æ‰¹é‡é¢„æµ‹
python main.py
#è¾“å‡ºï¼š
'''
[{'source': 'å¯¹å¾…æ¯ä¸€é¡¹å·¥ä½œéƒ½è¦ä¸€ä¸ä¸å¤Ÿã€‚', 'target': 'å¯¹å¾…æ¯ä¸€é¡¹å·¥ä½œéƒ½è¦ä¸€ä¸ä¸è‹Ÿã€‚', 'errors': [('å¤Ÿ', 'è‹Ÿ', 12)]}, {'source': 'å¤§çº¦åŠä¸ªå°æ—¶å·¦å³', 'target': 'å¤§çº¦åŠä¸ªå°æ—¶', 'errors': [('å·¦å³', '', 6)]}]
'''
```


### Transformers æ‰¹é‡æ¨ç†

- Clone the repo

``` sh
git clone https://github.com/TW-NLP/ChineseErrorCorrector
cd ChineseErrorCorrector
```

- Install Conda: please see https://docs.conda.io/en/latest/miniconda.html
- Create Conda env:

``` sh
conda create -n zh_correct -y python=3.10
conda activate zh_correct
pip install -r requirements.txt
# If you are in mainland China, you can set the mirror as follows:
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
```

``` sh
# ä¿®æ”¹config.py
#ï¼ˆ1ï¼‰æ ¹æ®ä¸åŒçš„æ¨¡å‹ï¼Œä¿®æ”¹çš„DEFAULT_CKPT_PATHï¼Œé»˜è®¤ä¸ºtwnlp/ChineseErrorCorrector3-4B
#ï¼ˆ2ï¼‰å°†TextCorrectConfigçš„USE_VLLM = False

#æ‰¹é‡é¢„æµ‹
python main.py

```

### ğŸ¤– modelscope

```shell
pip install modelscope
```

```shell
from modelscope import AutoModelForCausalLM, AutoTokenizer

model_name = "tiannlp/twnlp/ChineseErrorCorrector3-4B"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬çº é”™ä¸“å®¶ï¼Œçº æ­£è¾“å…¥å¥å­ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶è¾“å‡ºæ­£ç¡®çš„å¥å­ï¼Œè¾“å…¥å¥å­ä¸ºï¼š"
text_input = "å¯¹å¾…æ¯ä¸€é¡¹å·¥ä½œéƒ½è¦ä¸€ä¸ä¸å¤Ÿã€‚"
messages = [
    {"role": "user", "content": prompt + text_input}
]
text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.
    )
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)

```


## Citation

If this work is helpful, please kindly cite as:

```bibtex

@inproceedings{wei2024ä¸­å°å­¦ä½œæ–‡è¯­æ³•é”™è¯¯æ£€æµ‹,
  title={ä¸­å°å­¦ä½œæ–‡è¯­æ³•é”™è¯¯æ£€æµ‹, ç—…å¥æ”¹å†™ä¸æµç•…æ€§è¯„çº§çš„è‡ªåŠ¨åŒ–æ–¹æ³•ç ”ç©¶},
  author={Wei, Tian},
  booktitle={Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 3: Evaluations)},
  pages={278--284},
  year={2024}
}
```
## Contact
-å¾®ä¿¡æˆ‘ï¼šå¤‡æ³¨ï¼šå§“å-å…¬å¸å/å­¦æ ¡å-NLP
<img src="[https://github.com/shibing624/pycorrector/blob/master/docs/git_image/wechat.jpeg](https://github.com/TW-NLP/ChineseErrorCorrector/blob/main/images/we_chat.jpg)" width="200" />

## References

* [ä¸­æ–‡çº é”™ç³»ç»Ÿ](https://github.com/shibing624/pycorrector)
* [çº é”™è®ºæ–‡](https://github.com/nghuyong/Chinese-text-correction-papers)
* [çº é”™è¯„æµ‹](https://github.com/open-writing-evaluation/jp_errant_bea)
